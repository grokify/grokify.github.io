# LLM Evaluations

Leverage the power of large language models to evaluate your generative model or application for hallucinations, toxicity, relevance of retrieved documents, and more.

Note:

* For the lab portion of this course select any 3 of the 6 labs to run through.
* Some notebooks will leverage OpenAI API keys and others will leverage Google API keys
* Some notebooks will leverage LangChain, and others will leverage LlamaIndex
* Some notebooks will use agents and others will use RAG tools.
* If you have any issues with the colab notebooks we recommend restarting your runtime, if more issues persist please reach out to #phoenix-support on the [Arize Community Slack page](https://arize.com/community/).

## Lab 1: Evaluating Hallucinations

* Evaluate the performance of an LLM-assisted approach to detecting hallucinations.
* Provide an experimental framework for users to iterate and improve on the default classification template.
* [Open in Colab](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb)
* [Open in GitHub](https://github.com/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_hallucination_classifications.ipynb)

## Lab 2: Evaluating Toxicity

* Evaluate the performance of an LLM-assisted toxic detection.
* Provide an experimental framework for users to iterate and improve on the default classification template.
* [Open in Colab](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_toxicity_classifications.ipynb)
* [Open in GitHub](https://github.com/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_toxicity_classifications.ipynb)

## Lab 3: Evaluating Relevance of Retrieved Documents

* Evaluate the performance of an LLM-assisted approach to relevance classification against information retrieval datasets with ground-truth relevance labels,
* Provide an experimental framework for users to iterate and improve on the default classification template.
* [Open in Colab](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb)
* [Open in GitHub](https://github.com/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_relevance_classifications.ipynb)

## Lab 4: Evaluating Question-Answering

* Evaluate the performance of an LLM-assisted approach to detecting issues with Q&A systems on retrieved context data.
* Provide an experimental framework for users to iterate and improve on the default classification template.
* [Open in Colab](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb)
* [Open in GitHub](https://github.com/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_QA_classifications.ipynb)

## Lab 5 Evaluating Summarization

* Evaluate the performance of an LLM-assisted approach to evaluating summarization quality,
* Provide an experimental framework for users to iterate and improve on the default classification template.
* [Open in Colab](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb)
* [Open in GitHub](https://github.com/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_summarization_classifications.ipynb)

## Lab 6: Evaluating Code Readability

* Evaluate the performance of an LLM-assisted approach to classifying generated code as readable or unreadable using datasets with ground-truth labels
* Provide an experimental framework for users to iterate and improve on the default classification template.
* [Open in Colab](https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb)
* [Open in GitHub](https://github.com/Arize-ai/phoenix/blob/main/tutorials/evals/evaluate_code_readability_classifications.ipynb)

## Videos

1. [Benchmarking Retrieval for RAG](https://www.youtube.com/watch?v=eLXivBehPGo&t=1s)
1. [LLM Summarization Evaluations: Statistical Analysis](https://www.youtube.com/watch?v=Y4b-NmzqDyk)
1. [LLM Hallucination Evaluation: Statistical Analysis](https://www.youtube.com/watch?v=IH45ltIMC3k)
